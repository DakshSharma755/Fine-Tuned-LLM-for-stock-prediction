{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fa944df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path\n",
    "# This allows the notebook to find the 'src' module\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "768ff6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True. Using GPU: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Setup and Imports ---\n",
    "# Make sure you've installed everything from req.txt\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Import your custom data scraper\n",
    "from src.data_ingestion.scraper import get_stock_data\n",
    "\n",
    "# Ensure CUDA is available\n",
    "if not torch.cuda.is_available():\n",
    "    raise SystemError(\"CUDA is not available. This script requires a GPU.\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}. Using GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14aada17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Configuration ---\n",
    "\n",
    "# The finalized Hugging Face model ID\n",
    "BASE_MODEL_ID = \"chuanli11/Llama-3.2-3B-Instruct-uncensored\"\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "\n",
    "# Directory to save the original, untouched model\n",
    "INITIAL_MODEL_PATH = os.path.join(PROJECT_ROOT, \"models\", \"initial\", f\"{BASE_MODEL_ID.replace('/', '_')}\")\n",
    "# Directory to save the fine-tuned adapters and training checkpoints\n",
    "PROCESSED_MODEL_PATH = os.path.join(PROJECT_ROOT, \"models\", \"processed\", f\"{BASE_MODEL_ID.replace('/', '_')}-timeseries-v1\")\n",
    "\n",
    "# Stock to train on\n",
    "STOCK_YFINANCE_LINK = \"https://finance.yahoo.com/quote/NVDA\"\n",
    "DATA_PERIOD = \"3y\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fa7d7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "110eb3df-9a90-4bcc-988d-a9b4e6ec1a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model 'chuanli11/Llama-3.2-3B-Instruct-uncensored' directly from Hugging Face with quantization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b11161a9f1484ca5f2606eb50d7e94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading base model '{BASE_MODEL_ID}' directly from Hugging Face with quantization...\")\n",
    "\n",
    "# QLoRA configuration using bitsandbytes\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load the base model directly from the Hub with all optimizations applied at once\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
    "tokenizer.pad_token = tokenizer.eos_token # Set padding token for training\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2b22020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data ingestion for link: https://finance.yahoo.com/quote/NVDA\n",
      "Extracted ticker: NVDA\n",
      "Successfully scraped 752 data points.\n",
      "Minor preprocessing complete. (Date formatting, 20-day MA calculated)\n",
      "Data temporarily saved to: data/processed/NVDA_processed_data.csv\n",
      "Created 669 training prompts.\n",
      "\n",
      "Example of the new, richer prompt:\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "You are a financial analyst specializing in time series forecasting.\n",
      "\n",
      "Below is an example of a forecast.\n",
      "---\n",
      "EXAMPLE:\n",
      "TASK: Time Series Forecast.\n",
      "STOCK: AAPL\n",
      "STATISTICS: mean=175.30, std_dev=5.40, trend=upward\n",
      "CONTEXT_DAYS: 3\n",
      "DATA: [170.10, 172.50, 176.80]\n",
      "Predict the next 2 closing prices.\n",
      "\n",
      "PREDICTION: [178.20, 177.90]\n",
      "---\n",
      "\n",
      "Now, perform the following task.\n",
      "\n",
      "TASK: Time Series Forecast.\n",
      "STOCK: NVDA\n",
      "STATISTICS: mean=15.45, std_dev=1.44, trend=upward\n",
      "CONTEXT_DAYS: 60\n",
      "DATA: [12.45, 12.58, 13.25, 12.88, 13.16, 13.82, 13.48, 13.53, 13.20, 13.40, 14.14, 14.28, 14.58, 13.76, 15.73, 16.31, 16.28, 16.65, 15.89, 15.66, 15.39, 15.30, 16.02, 16.50, 16.25, 15.81, 15.62, 16.91, 17.12, 16.86, 16.59, 15.97, 16.10, 17.15, 16.98, 17.52, 18.05, 17.66, 16.94, 16.56, 16.24, 16.07, 16.49, 15.32, 15.19, 14.11, 14.02, 14.59, 14.60, 14.30, 14.74, 14.25, 14.84, 15.61, 15.89, 15.99, 16.50, 16.88, 17.69, 17.36]\n",
      "Predict the next 5 closing prices.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "PREDICTION: [16.75, 17.82, 19.17, 19.25, 19.30]<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Ingest and Prepare Data (IMPROVED VERSION) ---\n",
    "\n",
    "# Fetch data for fine-tuning\n",
    "df = get_stock_data(yfinance_link=STOCK_YFINANCE_LINK, period=DATA_PERIOD)\n",
    "# Extract the ticker symbol for use in the prompt\n",
    "ticker_symbol = STOCK_YFINANCE_LINK.split(\"/\")[-1]\n",
    "\n",
    "# --- Define the new, more advanced formatting function ---\n",
    "def format_training_prompt(row, context_days=60, prediction_days=5):\n",
    "    \"\"\"\n",
    "    Creates a rich text prompt with statistical context and a one-shot example.\n",
    "    \"\"\"\n",
    "    current_index = row.name\n",
    "    if current_index < context_days:\n",
    "        return None\n",
    "    \n",
    "    prediction_end_index = current_index + prediction_days\n",
    "    if prediction_end_index > len(df):\n",
    "        return None\n",
    "    \n",
    "    # --- 1. Get Data Slices ---\n",
    "    context_df = df.iloc[current_index - context_days : current_index]\n",
    "    context_prices = context_df['Close'].tolist()\n",
    "    prediction_prices = df.iloc[current_index : prediction_end_index]['Close'].tolist()\n",
    "\n",
    "    # --- 2. Calculate Statistics (new) ---\n",
    "    mean = context_df['Close'].mean()\n",
    "    std_dev = context_df['Close'].std()\n",
    "    trend = \"upward\" if context_prices[-1] > context_prices[0] else \"downward\"\n",
    "\n",
    "    # --- 3. Format data into strings ---\n",
    "    context_str = \", \".join([f\"{p:.2f}\" for p in context_prices])\n",
    "    prediction_str = \", \".join([f\"{p:.2f}\" for p in prediction_prices])\n",
    "\n",
    "    # --- 4. Create the Rich Prompt (new) ---\n",
    "    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "You are a financial analyst specializing in time series forecasting.\n",
    "\n",
    "Below is an example of a forecast.\n",
    "---\n",
    "EXAMPLE:\n",
    "TASK: Time Series Forecast.\n",
    "STOCK: AAPL\n",
    "STATISTICS: mean=175.30, std_dev=5.40, trend=upward\n",
    "CONTEXT_DAYS: 3\n",
    "DATA: [170.10, 172.50, 176.80]\n",
    "Predict the next 2 closing prices.\n",
    "\n",
    "PREDICTION: [178.20, 177.90]\n",
    "---\n",
    "\n",
    "Now, perform the following task.\n",
    "\n",
    "TASK: Time Series Forecast.\n",
    "STOCK: {ticker_symbol}\n",
    "STATISTICS: mean={mean:.2f}, std_dev={std_dev:.2f}, trend={trend}\n",
    "CONTEXT_DAYS: {context_days}\n",
    "DATA: [{context_str}]\n",
    "Predict the next {prediction_days} closing prices.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "PREDICTION: [{prediction_str}]<|eot_id|>\"\"\"\n",
    "    \n",
    "    return {\"text\": prompt}\n",
    "\n",
    "# --- Create the dataset ---\n",
    "prompts = df.apply(format_training_prompt, axis=1).dropna().tolist()\n",
    "dataset = Dataset.from_list(prompts)\n",
    "print(f\"Created {len(dataset)} training prompts.\")\n",
    "print(\"\\nExample of the new, richer prompt:\")\n",
    "print(dataset[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30ff098a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4151/975420769.py:40: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Configure PEFT (LoRA) and Trainer ---\n",
    "\n",
    "# Prepare the model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=32, # Rank of the update matrices. Lower is smaller, faster, but less expressive.\n",
    "    lora_alpha=32, # Alpha parameter for scaling.\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # Target modules can vary by model, you may need to experiment\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    ")\n",
    "\n",
    "# Add LoRA adapters to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# --- Training Arguments ---\n",
    "# This is where we configure checkpointing\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=PROCESSED_MODEL_PATH,\n",
    "    per_device_train_batch_size=1, # Keep this low for 4GB VRAM\n",
    "    gradient_accumulation_steps=4, # Simulate a larger batch size\n",
    "    learning_rate=2e-4,\n",
    "    max_grad_norm=0.3,\n",
    "    num_train_epochs=3, # Start with 1 epoch, you can increase later\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    warmup_ratio=0.03,\n",
    "    logging_steps=25, # Log progress every 25 steps\n",
    "    save_strategy=\"steps\", # Enable saving based on steps\n",
    "    save_steps=100, # Save a checkpoint every 100 steps\n",
    "    save_total_limit=3, # Only keep the last 3 checkpoints\n",
    "    bf16=True, # Use bfloat16 for training if your GPU supports it (Ampere series like RTX 3050 Ti does)\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# --- Initialize Trainer ---\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=lambda data: {'input_ids': torch.stack([tokenizer(sample['text'], return_tensors=\"pt\").input_ids.squeeze(0) for sample in data]),\n",
    "                                'labels': torch.stack([tokenizer(sample['text'], return_tensors=\"pt\").input_ids.squeeze(0) for sample in data])}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bee3708-adf3-4c15-8772-09a2e49f9c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_utils import get_last_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19502f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/anonion/ftmodel/.vnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='504' max='504' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [504/504 1:45:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.094900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.731500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.237800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.087900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.068000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.063400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.060100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.055800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.058000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.055900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.054700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.053000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.054800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.050500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.047800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.048000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.051200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.049400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.049900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.048900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anonion/ftmodel/.vnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/anonion/ftmodel/.vnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/anonion/ftmodel/.vnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/anonion/ftmodel/.vnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/anonion/ftmodel/.vnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "Final fine-tuned model adapters saved to: /home/anonion/ftmodel/models/processed/chuanli11_Llama-3.2-3B-Instruct-uncensored-timeseries-v1/final_model\n"
     ]
    }
   ],
   "source": [
    "last_checkpoint = get_last_checkpoint(PROCESSED_MODEL_PATH)\n",
    "\n",
    "print(f\"Starting training...\")\n",
    "if last_checkpoint:\n",
    "    print(f\"Resuming from checkpoint: {last_checkpoint}\")\n",
    "\n",
    "# The trainer will now start fresh if last_checkpoint is None, \n",
    "# or resume from the path if a checkpoint is found.\n",
    "trainer.train(resume_from_checkpoint=last_checkpoint)\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# --- Save the final model adapter ---\n",
    "final_model_path = os.path.join(PROCESSED_MODEL_PATH, \"final_model\")\n",
    "trainer.save_model(final_model_path)\n",
    "\n",
    "print(f\"Final fine-tuned model adapters saved to: {final_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77abce99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c787ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a013049",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1879c39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e288fc83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e60bb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffddb7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c12fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927d3ed7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be0689d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f849a696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2414b443",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92306fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
