INFO:     Started server process [72239]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8069 (Press CTRL+C to quit)
2025-10-16T08:49:19.233Z - INFO - User requested 7 days. Model will predict 7 days using 90 days of context.
2025-10-16T08:49:20.561Z - INFO - --- Loading and Running Instance 1a: Context Summarization (Llama) ---
2025-10-16T08:49:22.933Z - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Starting data ingestion for link: https://finance.yahoo.com/quote/AAPL
Extracted ticker: AAPL
Successfully scraped 1255 data points.
Minor preprocessing complete. (Date formatting, 20-day MA calculated)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  8.23s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  9.04s/it]
2025-10-16T08:49:53.922Z - INFO - Final Historical Summary: 75, 226.49, 227.63, 225.94, 225.91, 227.18, 227.76, 227.63, 224.55, 225.81, 223.64, 223.19, 226.42, 223.49, 224.64, 227.01, 230.12, 228.54, 227.97, 229.49, 229.63, 230.15, 233.11, 236.18, 238.21, 236.35, 235.87, 235.57, 233.31, 233.84, 233.
2025-10-16T08:49:53.929Z - INFO - --- Releasing Summarization model ---
2025-10-16T08:49:53.971Z - INFO - --- Loading and Running Instance 1b: Time Series Forecasting (Fine-tuned) ---
2025-10-16T08:49:54.501Z - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-10-16T08:50:03.490Z - WARNING - Statistically unlikely jump detected: [31.0, 228.31, 228.09, 228.33, 229.32, 228.71, 228.76]. Flagging as unreliable.
2025-10-16T08:50:03.505Z - INFO - --- Releasing Time Series model ---
2025-10-16T08:50:03.556Z - INFO - --- Loading and Running Instance 2a: FinBERT Labeling ---
Device set to use cuda:0
2025-10-16T08:50:06.390Z - INFO - Searching for news with query: "Apple Inc." OR AAPL
2025-10-16T08:50:08.446Z - INFO - --- Releasing FinBERT model ---
2025-10-16T08:50:08.453Z - INFO - --- Loading and Running Instances 2b & 3: Llama Synthesis ---
`torch_dtype` is deprecated! Use `dtype` instead!
2025-10-16T08:50:09.629Z - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:49<00:49, 49.95s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:11<00:00, 33.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:11<00:00, 35.87s/it]
2025-10-16T08:51:46.590Z - INFO - --- Releasing all models from memory ---
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   2274 MiB |   3377 MiB | 301590 MiB | 299316 MiB |
|       from large pool |   2231 MiB |   3345 MiB | 264900 MiB | 262669 MiB |
|       from small pool |     42 MiB |    130 MiB |  36689 MiB |  36647 MiB |
|---------------------------------------------------------------------------|
| Active memory         |   2274 MiB |   3377 MiB | 301590 MiB | 299316 MiB |
|       from large pool |   2231 MiB |   3345 MiB | 264900 MiB | 262669 MiB |
|       from small pool |     42 MiB |    130 MiB |  36689 MiB |  36647 MiB |
|---------------------------------------------------------------------------|
| Requested memory      |   2271 MiB |   3375 MiB | 298217 MiB | 295945 MiB |
|       from large pool |   2229 MiB |   3343 MiB | 261648 MiB | 259419 MiB |
|       from small pool |     42 MiB |    130 MiB |  36568 MiB |  36526 MiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   2514 MiB |   3546 MiB |   6140 MiB |   3626 MiB |
|       from large pool |   2444 MiB |   3440 MiB |   5884 MiB |   3440 MiB |
|       from small pool |     70 MiB |    134 MiB |    256 MiB |    186 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory | 241594 KiB |   1450 MiB | 422290 MiB | 422054 MiB |
|       from large pool | 217472 KiB |   1418 MiB | 381347 MiB | 381135 MiB |
|       from small pool |  24122 KiB |     49 MiB |  40942 MiB |  40918 MiB |
|---------------------------------------------------------------------------|
| Allocations           |     651    |    1357    |    1213 K  |    1213 K  |
|       from large pool |     282    |     424    |      92 K  |      92 K  |
|       from small pool |     369    |    1276    |    1121 K  |    1121 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     651    |    1357    |    1213 K  |    1213 K  |
|       from large pool |     282    |     424    |      92 K  |      92 K  |
|       from small pool |     369    |    1276    |    1121 K  |    1121 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      51    |      84    |     152    |     101    |
|       from large pool |      16    |      17    |      24    |       8    |
|       from small pool |      35    |      67    |     128    |      93    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     107    |     114    |  553324    |  553217    |
|       from large pool |      10    |      19    |   34766    |   34756    |
|       from small pool |      97    |     106    |  518558    |  518461    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

INFO:     127.0.0.1:38404 - "POST /analyze HTTP/1.1" 200 OK
INFO:     Started server process [119340]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8069 (Press CTRL+C to quit)
2025-10-16T09:17:10.625Z - INFO - User requested 7 days. Model will predict 7 days using 90 days of context.
2025-10-16T09:17:11.547Z - INFO - --- Loading and Running Instance 1a: Context Summarization (Llama) ---
2025-10-16T09:17:13.580Z - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Starting data ingestion for link: https://finance.yahoo.com/quote/AAPL
Extracted ticker: AAPL
Successfully scraped 1255 data points.
Minor preprocessing complete. (Date formatting, 20-day MA calculated)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:14<00:14, 14.76s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00,  9.67s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00, 10.43s/it]
2025-10-16T09:17:51.398Z - INFO - Final Historical Summary: 88, 224.51, 225.39, 226.15, 223.90, 224.25, 226.04, 224.45, 222.85, 221.52, 223.10, 221.63, 219.92, 216.75, 213.35, 212.04, 210.55, 209.90, 208.39, 208.87, 207.29, 206.51, 204.89, 203.57, 202.09, 201.47, 200.82, 200.00, 199.48, 198.73, 198.
2025-10-16T09:17:51.438Z - INFO - --- Releasing Summarization model ---
2025-10-16T09:17:51.559Z - INFO - --- Loading and Running Instance 1b: Time Series Forecasting (Fine-tuned) ---
2025-10-16T09:17:53.430Z - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-10-16T09:18:05.434Z - WARNING - Statistically unlikely jump detected: [90.0, 222.92, 222.98, 221.82, 222.92, 223.92, 223.68]. Flagging as unreliable.
2025-10-16T09:18:05.454Z - INFO - --- Releasing Time Series model ---
2025-10-16T09:18:05.517Z - INFO - --- Loading and Running Instance 2a: FinBERT Labeling ---
Device set to use cuda:0
2025-10-16T09:18:08.160Z - INFO - Searching for news with query: "Apple Inc." OR AAPL
2025-10-16T09:18:09.555Z - INFO - --- Releasing FinBERT model ---
2025-10-16T09:18:09.563Z - INFO - --- Loading and Running Instances 2b & 3: Llama Synthesis ---
`torch_dtype` is deprecated! Use `dtype` instead!
2025-10-16T09:18:09.948Z - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:46<00:46, 46.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:04<00:00, 30.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:04<00:00, 32.49s/it]
2025-10-16T09:19:43.716Z - INFO - --- Releasing all models from memory ---
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   2273 MiB |   3377 MiB | 302283 MiB | 300010 MiB |
|       from large pool |   2230 MiB |   3345 MiB | 265593 MiB | 263362 MiB |
|       from small pool |     42 MiB |    130 MiB |  36689 MiB |  36647 MiB |
|---------------------------------------------------------------------------|
| Active memory         |   2273 MiB |   3377 MiB | 302283 MiB | 300010 MiB |
|       from large pool |   2230 MiB |   3345 MiB | 265593 MiB | 263362 MiB |
|       from small pool |     42 MiB |    130 MiB |  36689 MiB |  36647 MiB |
|---------------------------------------------------------------------------|
| Requested memory      |   2271 MiB |   3375 MiB | 298382 MiB | 296110 MiB |
|       from large pool |   2229 MiB |   3343 MiB | 261813 MiB | 259583 MiB |
|       from small pool |     42 MiB |    130 MiB |  36568 MiB |  36526 MiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   2508 MiB |   3546 MiB |   6038 MiB |   3530 MiB |
|       from large pool |   2436 MiB |   3440 MiB |   5732 MiB |   3296 MiB |
|       from small pool |     72 MiB |    136 MiB |    306 MiB |    234 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory | 142266 KiB |   1542 MiB | 365538 MiB | 365400 MiB |
|       from large pool | 112000 KiB |   1533 MiB | 324013 MiB | 323904 MiB |
|       from small pool |  30266 KiB |     29 MiB |  41525 MiB |  41495 MiB |
|---------------------------------------------------------------------------|
| Allocations           |     651    |    1357    |    1213 K  |    1213 K  |
|       from large pool |     282    |     424    |      92 K  |      92 K  |
|       from small pool |     369    |    1276    |    1121 K  |    1121 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     651    |    1357    |    1213 K  |    1213 K  |
|       from large pool |     282    |     424    |      92 K  |      92 K  |
|       from small pool |     369    |    1276    |    1121 K  |    1121 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      61    |      94    |     183    |     122    |
|       from large pool |      25    |      26    |      30    |       5    |
|       from small pool |      36    |      68    |     153    |     117    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      95    |     106    |  547605    |  547510    |
|       from large pool |       5    |      21    |   31950    |   31945    |
|       from small pool |      90    |      98    |  515655    |  515565    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

INFO:     127.0.0.1:45478 - "POST /analyze HTTP/1.1" 200 OK
2025-10-16T09:35:29.696Z - INFO - User requested 7 days. Model will predict 7 days using 90 days of context.
2025-10-16T09:35:30.656Z - INFO - --- Loading and Running Instance 1a: Context Summarization (Llama) ---
2025-10-16T09:35:32.980Z - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Starting data ingestion for link: https://finance.yahoo.com/quote/AAPL
Extracted ticker: AAPL
Successfully scraped 1255 data points.
Minor preprocessing complete. (Date formatting, 20-day MA calculated)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.64s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.75s/it]
2025-10-16T09:36:03.346Z - INFO - Final Historical Summary: 227.17, 224.59, 224.90, 223.79, 222.21, 224.53, 225.57, 223.39, 221.55, 220.85, 222.30, 224.42, 225.52, 224.79, 226.17, 226.63, 225.85, 227.19, 228.33, 228.48, 227.59, 225.92, 225.08, 227.42, 228.15, 226.87, 224.39, 223.69, 225.23, 225.
2025-10-16T09:36:03.352Z - INFO - --- Releasing Summarization model ---
2025-10-16T09:36:03.406Z - INFO - --- Loading and Running Instance 1b: Time Series Forecasting (Fine-tuned) ---
2025-10-16T09:36:03.971Z - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-10-16T09:36:13.439Z - INFO - --- Releasing Time Series model ---
2025-10-16T09:36:13.490Z - INFO - --- Loading and Running Instance 2a: FinBERT Labeling ---
Device set to use cuda:0
2025-10-16T09:36:17.107Z - INFO - Searching for news with query: "Apple Inc." OR AAPL
2025-10-16T09:36:18.332Z - INFO - --- Releasing FinBERT model ---
2025-10-16T09:36:18.343Z - INFO - --- Loading and Running Instances 2b & 3: Llama Synthesis ---
`torch_dtype` is deprecated! Use `dtype` instead!
2025-10-16T09:36:18.740Z - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:51<00:51, 51.95s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:16<00:00, 35.71s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:16<00:00, 38.15s/it]
2025-10-16T09:38:10.395Z - INFO - --- Releasing all models from memory ---
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   2273 MiB |   3377 MiB | 298849 MiB | 296576 MiB |
|       from large pool |   2230 MiB |   3345 MiB | 260935 MiB | 258704 MiB |
|       from small pool |     42 MiB |    130 MiB |  37914 MiB |  37871 MiB |
|---------------------------------------------------------------------------|
| Active memory         |   2273 MiB |   3377 MiB | 298849 MiB | 296576 MiB |
|       from large pool |   2230 MiB |   3345 MiB | 260935 MiB | 258704 MiB |
|       from small pool |     42 MiB |    130 MiB |  37914 MiB |  37871 MiB |
|---------------------------------------------------------------------------|
| Requested memory      |   2271 MiB |   3375 MiB | 294996 MiB | 292724 MiB |
|       from large pool |   2229 MiB |   3343 MiB | 257201 MiB | 254972 MiB |
|       from small pool |     42 MiB |    130 MiB |  37794 MiB |  37752 MiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   2508 MiB |   3546 MiB |   6038 MiB |   3530 MiB |
|       from large pool |   2436 MiB |   3440 MiB |   5732 MiB |   3296 MiB |
|       from small pool |     72 MiB |    136 MiB |    306 MiB |    234 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory | 142268 KiB |   1542 MiB | 362399 MiB | 362260 MiB |
|       from large pool | 112000 KiB |   1533 MiB | 319355 MiB | 319246 MiB |
|       from small pool |  30268 KiB |     29 MiB |  43043 MiB |  43013 MiB |
|---------------------------------------------------------------------------|
| Allocations           |     651    |    1357    |    1201 K  |    1201 K  |
|       from large pool |     282    |     424    |      89 K  |      89 K  |
|       from small pool |     369    |    1276    |    1112 K  |    1111 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     651    |    1357    |    1201 K  |    1201 K  |
|       from large pool |     282    |     424    |      89 K  |      89 K  |
|       from small pool |     369    |    1276    |    1112 K  |    1111 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      61    |      94    |     183    |     122    |
|       from large pool |      25    |      26    |      30    |       5    |
|       from small pool |      36    |      68    |     153    |     117    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      96    |     107    |  544921    |  544825    |
|       from large pool |       5    |      21    |   31749    |   31744    |
|       from small pool |      91    |      99    |  513172    |  513081    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

INFO:     127.0.0.1:40582 - "POST /analyze HTTP/1.1" 200 OK
2025-10-16T09:51:13.379Z - INFO - User requested 7 days. Model will predict 7 days using 90 days of context.
2025-10-16T09:51:14.511Z - INFO - --- Loading and Running Instance 1a: Context Summarization (Llama) ---
2025-10-16T09:51:17.169Z - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Starting data ingestion for link: https://finance.yahoo.com/quote/AAPL
Extracted ticker: AAPL
Successfully scraped 1255 data points.
Minor preprocessing complete. (Date formatting, 20-day MA calculated)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.08s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:21<00:00, 10.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:21<00:00, 10.73s/it]
2025-10-16T09:51:49.703Z - INFO - Final Historical Summary: 224.11, 227.82, 228.57, 230.91, 231.83, 233.04, 236.32, 236.76, 236.56, 235.92, 239.15, 238.35, 237.49, 236.83, 238.63, 240.08, 241.14, 239.63, 240.51, 241.79, 244.51, 245.59, 245.11, 243.42, 241.83, 244.69, 245.36, 247.55, 248.12, 246.
2025-10-16T09:51:49.714Z - INFO - --- Releasing Summarization model ---
2025-10-16T09:51:49.775Z - INFO - --- Loading and Running Instance 1b: Time Series Forecasting (Fine-tuned) ---
2025-10-16T09:51:51.273Z - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-10-16T09:52:04.604Z - INFO - --- Releasing Time Series model ---
2025-10-16T09:52:04.655Z - INFO - --- Loading and Running Instance 2a: FinBERT Labeling ---
Device set to use cuda:0
2025-10-16T09:52:08.029Z - INFO - Searching for news with query: "Apple Inc." OR AAPL
2025-10-16T09:52:09.359Z - INFO - --- Releasing FinBERT model ---
2025-10-16T09:52:09.366Z - INFO - --- Loading and Running Instances 2b & 3: Llama Synthesis ---
`torch_dtype` is deprecated! Use `dtype` instead!
2025-10-16T09:52:09.766Z - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:50<00:50, 50.22s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 34.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 36.75s/it]
2025-10-16T09:53:50.666Z - INFO - --- Releasing all models from memory ---
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   2273 MiB |   3377 MiB | 300555 MiB | 298282 MiB |
|       from large pool |   2230 MiB |   3345 MiB | 262850 MiB | 260619 MiB |
|       from small pool |     42 MiB |    130 MiB |  37704 MiB |  37662 MiB |
|---------------------------------------------------------------------------|
| Active memory         |   2273 MiB |   3377 MiB | 300555 MiB | 298282 MiB |
|       from large pool |   2230 MiB |   3345 MiB | 262850 MiB | 260619 MiB |
|       from small pool |     42 MiB |    130 MiB |  37704 MiB |  37662 MiB |
|---------------------------------------------------------------------------|
| Requested memory      |   2271 MiB |   3375 MiB | 296684 MiB | 294412 MiB |
|       from large pool |   2229 MiB |   3343 MiB | 259101 MiB | 256871 MiB |
|       from small pool |     42 MiB |    130 MiB |  37583 MiB |  37541 MiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   2508 MiB |   3546 MiB |   6038 MiB |   3530 MiB |
|       from large pool |   2436 MiB |   3440 MiB |   5732 MiB |   3296 MiB |
|       from small pool |     72 MiB |    136 MiB |    306 MiB |    234 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory | 142266 KiB |   1542 MiB | 364108 MiB | 363969 MiB |
|       from large pool | 112000 KiB |   1533 MiB | 321270 MiB | 321161 MiB |
|       from small pool |  30266 KiB |     29 MiB |  42837 MiB |  42807 MiB |
|---------------------------------------------------------------------------|
| Allocations           |     651    |    1357    |    1213 K  |    1213 K  |
|       from large pool |     282    |     424    |      90 K  |      90 K  |
|       from small pool |     369    |    1276    |    1123 K  |    1122 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     651    |    1357    |    1213 K  |    1213 K  |
|       from large pool |     282    |     424    |      90 K  |      90 K  |
|       from small pool |     369    |    1276    |    1123 K  |    1122 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      61    |      94    |     183    |     122    |
|       from large pool |      25    |      26    |      30    |       5    |
|       from small pool |      36    |      68    |     153    |     117    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      94    |     105    |  559006    |  558912    |
|       from large pool |       5    |      21    |   31821    |   31816    |
|       from small pool |      89    |      97    |  527185    |  527096    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

INFO:     127.0.0.1:42590 - "POST /analyze HTTP/1.1" 200 OK
2025-10-16T09:55:07.009Z - INFO - User requested 30 days. Model will predict 30 days using 365 days of context.
2025-10-16T09:55:07.980Z - INFO - --- Loading and Running Instance 1a: Context Summarization (Llama) ---
2025-10-16T09:55:10.652Z - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Starting data ingestion for link: https://finance.yahoo.com/quote/AAPL
Extracted ticker: AAPL
Successfully scraped 1255 data points.
Minor preprocessing complete. (Date formatting, 20-day MA calculated)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:48<00:48, 48.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:12<00:00, 34.28s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:12<00:00, 36.39s/it]
2025-10-16T09:56:58.182Z - INFO - Final Historical Summary: 242.40, 241.35, 241.80, 242.30, 244.55, 245.70, 245.20, 245.85, 245.30, 246.00, 247.15, 248.10, 248.80, 249.30, 250.70, 251.40, 252.70, 253.50, 254.10, 254.80, 255.50, 256.20, 256.80, 257.50, 258.20, 258.80, 259.50, 260.20, 260.80, 261.
2025-10-16T09:56:58.193Z - INFO - --- Releasing Summarization model ---
2025-10-16T09:56:58.257Z - INFO - --- Loading and Running Instance 1b: Time Series Forecasting (Fine-tuned) ---
2025-10-16T09:56:59.177Z - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-10-16T09:57:34.543Z - WARNING - Statistically unlikely jump detected: [50.0, 203.99, 199.0, 198.25, 198.7, 195.0, 198.0, 198.0, 198.0, 198.0, 198.0, 198.0, 199.0, 199.0, 199.0, 199.0, 199.0, 199.0, 199.0, 199.0, 199.0, 199.0, 199.0, 199.0, 199.0, 199.0, 199.0, 199.0, 199.0, 199.0]. Flagging as unreliable.
2025-10-16T09:57:34.560Z - INFO - --- Releasing Time Series model ---
2025-10-16T09:57:34.626Z - INFO - --- Loading and Running Instance 2a: FinBERT Labeling ---
Device set to use cuda:0
2025-10-16T09:57:42.404Z - INFO - Searching for news with query: "Apple Inc." OR AAPL
2025-10-16T09:57:43.763Z - INFO - --- Releasing FinBERT model ---
2025-10-16T09:57:43.770Z - INFO - --- Loading and Running Instances 2b & 3: Llama Synthesis ---
`torch_dtype` is deprecated! Use `dtype` instead!
2025-10-16T09:57:44.200Z - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:59<00:59, 59.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:29<00:00, 41.82s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:29<00:00, 44.55s/it]
2025-10-16T09:59:45.320Z - INFO - --- Releasing all models from memory ---
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   2274 MiB |   3377 MiB |    929 GiB |    927 GiB |
|       from large pool |   2231 MiB |   3345 MiB |    842 GiB |    840 GiB |
|       from small pool |     42 MiB |    130 MiB |     87 GiB |     87 GiB |
|---------------------------------------------------------------------------|
| Active memory         |   2274 MiB |   3377 MiB |    929 GiB |    927 GiB |
|       from large pool |   2231 MiB |   3345 MiB |    842 GiB |    840 GiB |
|       from small pool |     42 MiB |    130 MiB |     87 GiB |     87 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |   2271 MiB |   3375 MiB |    913 GiB |    911 GiB |
|       from large pool |   2229 MiB |   3343 MiB |    826 GiB |    823 GiB |
|       from small pool |     42 MiB |    130 MiB |     87 GiB |     87 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   2514 MiB |   3546 MiB |   6140 MiB |   3626 MiB |
|       from large pool |   2444 MiB |   3440 MiB |   5884 MiB |   3440 MiB |
|       from small pool |     70 MiB |    134 MiB |    256 MiB |    186 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory | 241591 KiB |   1450 MiB |   1086 GiB |   1086 GiB |
|       from large pool | 217472 KiB |   1418 MiB |    997 GiB |    997 GiB |
|       from small pool |  24119 KiB |     44 MiB |     88 GiB |     88 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     651    |    1357    |    2125 K  |    2124 K  |
|       from large pool |     282    |     424    |     292 K  |     292 K  |
|       from small pool |     369    |    1276    |    1832 K  |    1832 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     651    |    1357    |    2125 K  |    2124 K  |
|       from large pool |     282    |     424    |     292 K  |     292 K  |
|       from small pool |     369    |    1276    |    1832 K  |    1832 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      51    |      84    |     152    |     101    |
|       from large pool |      16    |      17    |      24    |       8    |
|       from small pool |      35    |      67    |     128    |      93    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     107    |     114    |     943 K  |     943 K  |
|       from large pool |      10    |      19    |     122 K  |     122 K  |
|       from small pool |      97    |     104    |     821 K  |     821 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

INFO:     127.0.0.1:49486 - "POST /analyze HTTP/1.1" 200 OK
2025-10-16T10:06:45.059Z - INFO - TEST MODE: Predicting 30 days. Model will use 365 days of context to predict 30 days.
2025-10-16T10:06:46.051Z - INFO - --- Loading and Running Instance 1a: Context Summarization (Llama) ---
2025-10-16T10:06:48.105Z - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Starting data ingestion for link: https://finance.yahoo.com/quote/AAPL
Extracted ticker: AAPL
Successfully scraped 1255 data points.
Minor preprocessing complete. (Date formatting, 20-day MA calculated)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:50<00:50, 50.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 34.50s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:13<00:00, 36.87s/it]
2025-10-16T10:09:15.099Z - INFO - Final Historical Summary: 219.40, 222.60, 223.80, 226.10, 227.30, 227.95, 229.40, 233.85, 233.80, 231.55, 228.80, 225.70, 221.60, 219.30, 216.80, 214.80, 211.90, 209.80, 206.80, 204.20, 200.50, 198.80, 195.60, 193.40, 190.60, 188.40, 185.30, 182.40, 179.30, 176.
2025-10-16T10:09:15.111Z - INFO - --- Releasing Summarization model ---
2025-10-16T10:09:15.169Z - INFO - --- Loading and Running Instance 1b: Time Series Forecasting (Fine-tuned) ---
2025-10-16T10:09:15.928Z - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-10-16T10:09:51.550Z - WARNING - Statistically unlikely jump detected: [91.0, 239.31, 240.83, 240.3, 241.98, 240.11, 241.97, 241.78, 239.79, 241.89, 241.91, 239.99, 240.96, 241.93, 241.33, 241.28, 241.37, 239.77, 240.45, 239.88, 239.96, 240.45, 241.68, 239.32, 239.48, 241.11, 239.27, 239.31, 239.11, 241.78]. Flagging as unreliable.
2025-10-16T10:09:51.569Z - INFO - --- Releasing Time Series model ---
2025-10-16T10:09:51.635Z - INFO - --- [STEP 2] Evaluating forecast accuracy ---
2025-10-16T10:09:51.642Z - INFO - Forecast Accuracy --> 
            MAE: $15.07
            MAPE: 6.11%
            RMSE: $29.52
            Directional Accuracy: 48.28%
        
2025-10-16T10:09:51.642Z - INFO - --- Loading and Running Instance 2a: FinBERT Labeling ---
Device set to use cuda:0
2025-10-16T10:09:58.812Z - INFO - Searching for news with query: "Apple Inc." OR AAPL
2025-10-16T10:09:56.318Z - ERROR - Could not fetch news from NewsAPI: {'status': 'error', 'code': 'parameterInvalid', 'message': 'You are trying to request results too far in the past. Your plan permits you to request articles as far back as 2025-09-15, but you have requested 2025-08-29. You may need to upgrade to a paid plan.'}
Traceback (most recent call last):
  File "/home/anonion/ftmodel/src/pipeline/eval.py", line 246, in run_test_pipeline
    articles = newsapi.get_everything(q=search_query, from_param=from_date, language='en', sort_by='relevancy', page_size=20)['articles']
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/anonion/ftmodel/.vnv/lib/python3.12/site-packages/newsapi/newsapi_client.py", line 334, in get_everything
    raise NewsAPIException(r.json())
newsapi.newsapi_exception.NewsAPIException: {'status': 'error', 'code': 'parameterInvalid', 'message': 'You are trying to request results too far in the past. Your plan permits you to request articles as far back as 2025-09-15, but you have requested 2025-08-29. You may need to upgrade to a paid plan.'}
2025-10-16T10:09:56.688Z - INFO - --- Releasing FinBERT model ---
2025-10-16T10:09:56.712Z - ERROR - An error occurred during the test pipeline: cannot access local variable 'analyses' where it is not associated with a value
Traceback (most recent call last):
  File "/home/anonion/ftmodel/src/pipeline/eval.py", line 270, in run_test_pipeline
    del sentiment_pipeline, analyses
                            ^^^^^^^^
UnboundLocalError: cannot access local variable 'analyses' where it is not associated with a value
2025-10-16T10:09:56.714Z - INFO - --- Releasing all models from memory ---
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   8320 KiB |   3377 MiB |    976 GiB |    976 GiB |
|       from large pool |   8320 KiB |   3345 MiB |    887 GiB |    887 GiB |
|       from small pool |      0 KiB |    130 MiB |     89 GiB |     89 GiB |
|---------------------------------------------------------------------------|
| Active memory         |   8320 KiB |   3377 MiB |    976 GiB |    976 GiB |
|       from large pool |   8320 KiB |   3345 MiB |    887 GiB |    887 GiB |
|       from small pool |      0 KiB |    130 MiB |     89 GiB |     89 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |   8320 KiB |   3375 MiB |    958 GiB |    958 GiB |
|       from large pool |   8320 KiB |   3343 MiB |    869 GiB |    869 GiB |
|       from small pool |      0 KiB |    130 MiB |     88 GiB |     88 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1330 MiB |   3546 MiB |   4140 MiB |   2810 MiB |
|       from large pool |   1260 MiB |   3440 MiB |   3948 MiB |   2688 MiB |
|       from small pool |     70 MiB |    132 MiB |    192 MiB |    122 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory | 761728 KiB |   1451 MiB |   1070 GiB |   1069 GiB |
|       from large pool | 761728 KiB |   1418 MiB |    969 GiB |    969 GiB |
|       from small pool |      0 KiB |     49 MiB |    100 GiB |    100 GiB |
|---------------------------------------------------------------------------|
| Allocations           |       1    |    1357    |    2599 K  |    2599 K  |
|       from large pool |       1    |     363    |     307 K  |     307 K  |
|       from small pool |       0    |    1276    |    2292 K  |    2292 K  |
|---------------------------------------------------------------------------|
| Active allocs         |       1    |    1357    |    2599 K  |    2599 K  |
|       from large pool |       1    |     363    |     307 K  |     307 K  |
|       from small pool |       0    |    1276    |    2292 K  |    2292 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      38    |      74    |     106    |      68    |
|       from large pool |       3    |       8    |      10    |       7    |
|       from small pool |      35    |      66    |      96    |      61    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |     112    |    1232 K  |    1232 K  |
|       from large pool |       2    |      19    |     125 K  |     125 K  |
|       from small pool |       0    |     105    |    1106 K  |    1106 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

INFO:     127.0.0.1:35286 - "POST /test HTTP/1.1" 200 OK
2025-10-16T10:14:38.328Z - INFO - TEST MODE: Predicting 30 days. Model will use 365 days of context to predict 30 days.
2025-10-16T10:14:39.770Z - INFO - --- Loading and Running Instance 1a: Context Summarization (Llama) ---
2025-10-16T10:14:42.264Z - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Starting data ingestion for link: https://finance.yahoo.com/quote/AAPL
Extracted ticker: AAPL
Successfully scraped 1255 data points.
Minor preprocessing complete. (Date formatting, 20-day MA calculated)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [01:09<01:09, 69.70s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:36<00:00, 44.49s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:36<00:00, 48.28s/it]
2025-10-16T10:18:11.530Z - INFO - Final Historical Summary: 219.58, 217.70, 215.70, 214.60, 211.80, 208.90, 206.60, 204.50, 202.40, 200.30, 198.20, 195.10, 193.00, 190.90, 188.80, 186.70, 184.60, 182.50, 180.40, 178.30, 176.20, 174.10, 172.00, 169.90, 167.80, 165.70, 163.60, 161.50, 159.40, 157.
2025-10-16T10:18:11.549Z - INFO - --- Releasing Summarization model ---
2025-10-16T10:18:11.675Z - INFO - --- Loading and Running Instance 1b: Time Series Forecasting (Fine-tuned) ---
2025-10-16T10:18:12.799Z - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-10-16T10:19:15.014Z - WARNING - Statistically unlikely jump detected: [20.0, 237.4, 240.88, 242.51, 240.84, 240.27, 239.91, 238.15, 238.95, 240.51, 239.5, 240.2, 241.25, 241.85, 241.5, 240.0, 240.0, 240.0, 241.0, 239.0, 239.0, 240.5, 240.0, 241.0, 239.0, 241.0, 241.0, 241.0, 241.0, 241.0]. Flagging as unreliable.
2025-10-16T10:19:15.036Z - INFO - --- Releasing Time Series model ---
2025-10-16T10:19:15.181Z - INFO - --- [STEP 2] Evaluating forecast accuracy ---
2025-10-16T10:19:15.188Z - INFO - Forecast Accuracy --> 
            MAE: $17.17
            MAPE: 6.98%
            RMSE: $41.76
            Directional Accuracy: 34.48%
        
2025-10-16T10:19:15.190Z - INFO - --- Loading and Running Instance 2a: FinBERT Labeling ---
Device set to use cuda:0
2025-10-16T10:19:24.872Z - INFO - Searching for news with query: "Apple Inc." OR AAPL
2025-10-16T10:19:25.764Z - ERROR - Could not fetch news from NewsAPI: {'status': 'error', 'code': 'parameterInvalid', 'message': 'You are trying to request results too far in the past. Your plan permits you to request articles as far back as 2025-09-15, but you have requested 2025-08-29. You may need to upgrade to a paid plan.'}
Traceback (most recent call last):
  File "/home/anonion/ftmodel/src/pipeline/eval.py", line 246, in run_test_pipeline
    articles = newsapi.get_everything(q=search_query, from_param=from_date, language='en', sort_by='relevancy', page_size=20)['articles']
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/anonion/ftmodel/.vnv/lib/python3.12/site-packages/newsapi/newsapi_client.py", line 334, in get_everything
    raise NewsAPIException(r.json())
newsapi.newsapi_exception.NewsAPIException: {'status': 'error', 'code': 'parameterInvalid', 'message': 'You are trying to request results too far in the past. Your plan permits you to request articles as far back as 2025-09-15, but you have requested 2025-08-29. You may need to upgrade to a paid plan.'}
2025-10-16T10:19:26.353Z - INFO - --- Releasing FinBERT model ---
2025-10-16T10:19:26.384Z - ERROR - An error occurred during the test pipeline: cannot access local variable 'analyses' where it is not associated with a value
Traceback (most recent call last):
  File "/home/anonion/ftmodel/src/pipeline/eval.py", line 271, in run_test_pipeline
    if analyses:
       ^^^^^^^^
UnboundLocalError: cannot access local variable 'analyses' where it is not associated with a value
2025-10-16T10:19:26.388Z - INFO - --- Releasing all models from memory ---
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   8320 KiB |   3377 MiB |    976 GiB |    976 GiB |
|       from large pool |   8320 KiB |   3345 MiB |    887 GiB |    887 GiB |
|       from small pool |      0 KiB |    130 MiB |     89 GiB |     89 GiB |
|---------------------------------------------------------------------------|
| Active memory         |   8320 KiB |   3377 MiB |    976 GiB |    976 GiB |
|       from large pool |   8320 KiB |   3345 MiB |    887 GiB |    887 GiB |
|       from small pool |      0 KiB |    130 MiB |     89 GiB |     89 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |   8320 KiB |   3375 MiB |    958 GiB |    958 GiB |
|       from large pool |   8320 KiB |   3343 MiB |    869 GiB |    869 GiB |
|       from small pool |      0 KiB |    130 MiB |     88 GiB |     88 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   1330 MiB |   3546 MiB |   4140 MiB |   2810 MiB |
|       from large pool |   1260 MiB |   3440 MiB |   3948 MiB |   2688 MiB |
|       from small pool |     70 MiB |    132 MiB |    192 MiB |    122 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory | 761728 KiB |   1451 MiB |   1070 GiB |   1069 GiB |
|       from large pool | 761728 KiB |   1418 MiB |    969 GiB |    969 GiB |
|       from small pool |      0 KiB |     49 MiB |    100 GiB |    100 GiB |
|---------------------------------------------------------------------------|
| Allocations           |       1    |    1357    |    2599 K  |    2599 K  |
|       from large pool |       1    |     363    |     307 K  |     307 K  |
|       from small pool |       0    |    1276    |    2292 K  |    2292 K  |
|---------------------------------------------------------------------------|
| Active allocs         |       1    |    1357    |    2599 K  |    2599 K  |
|       from large pool |       1    |     363    |     307 K  |     307 K  |
|       from small pool |       0    |    1276    |    2292 K  |    2292 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      38    |      74    |     106    |      68    |
|       from large pool |       3    |       8    |      10    |       7    |
|       from small pool |      35    |      66    |      96    |      61    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       2    |     112    |    1232 K  |    1232 K  |
|       from large pool |       2    |      19    |     125 K  |     125 K  |
|       from small pool |       0    |     105    |    1106 K  |    1106 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

INFO:     127.0.0.1:48918 - "POST /test HTTP/1.1" 200 OK
2025-10-16T10:21:15.704Z - INFO - TEST MODE: Predicting 30 days. Model will use 365 days of context to predict 30 days.
2025-10-16T10:21:16.582Z - INFO - --- Loading and Running Instance 1a: Context Summarization (Llama) ---
2025-10-16T10:21:19.435Z - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Starting data ingestion for link: https://finance.yahoo.com/quote/AAPL
Extracted ticker: AAPL
Successfully scraped 1255 data points.
Minor preprocessing complete. (Date formatting, 20-day MA calculated)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [01:21<01:21, 81.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [02:06<00:00, 59.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [02:06<00:00, 63.18s/it]
2025-10-16T10:24:41.346Z - INFO - Final Historical Summary: 219.45, 219.65, 221.18, 223.39, 222.82, 225.50, 226.95, 227.09, 228.35, 228.55, 229.45, 229.55, 230.80, 232.10, 232.35, 233.55, 233.80, 233.85, 234.05, 234.20, 234.30, 234.40, 234.50, 234.60, 234.70, 234.80, 234.90, 234.95, 234.98, 235.
2025-10-16T10:24:41.364Z - INFO - --- Releasing Summarization model ---
2025-10-16T10:24:41.456Z - INFO - --- Loading and Running Instance 1b: Time Series Forecasting (Fine-tuned) ---
2025-10-16T10:24:42.614Z - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-10-16T10:25:24.645Z - WARNING - Statistically unlikely jump detected: [97.0, 239.39, 239.94, 239.91, 241.45, 239.92, 240.78, 241.92, 241.6, 240.08, 239.51, 240.79, 241.91, 239.79, 239.59, 239.97, 239.79, 241.98, 239.91, 240.77, 241.92, 240.76, 239.84, 239.9, 241.31, 241.32, 241.88, 241.76, 240.19, 241.91]. Flagging as unreliable.
2025-10-16T10:25:24.671Z - INFO - --- Releasing Time Series model ---
2025-10-16T10:25:24.787Z - INFO - --- [STEP 2] Evaluating forecast accuracy ---
2025-10-16T10:25:24.796Z - INFO - Forecast Accuracy --> 
            MAE: $14.46
            MAPE: 5.86%
            RMSE: $28.42
            Directional Accuracy: 51.72%
        
2025-10-16T10:25:24.797Z - INFO - --- Loading and Running Instance 2a: FinBERT Labeling ---
Device set to use cuda:0
2025-10-16T10:25:33.410Z - INFO - Searching for news with query: "Apple Inc." OR AAPL
2025-10-16T10:25:34.090Z - ERROR - Could not fetch news from NewsAPI: {'status': 'error', 'code': 'parameterInvalid', 'message': 'You are trying to request results too far in the past. Your plan permits you to request articles as far back as 2025-09-15, but you have requested 2025-08-29. You may need to upgrade to a paid plan.'}
Traceback (most recent call last):
  File "/home/anonion/ftmodel/src/pipeline/eval.py", line 246, in run_test_pipeline
    articles = newsapi.get_everything(q=search_query, from_param=from_date, language='en', sort_by='relevancy', page_size=20)['articles']
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/anonion/ftmodel/.vnv/lib/python3.12/site-packages/newsapi/newsapi_client.py", line 334, in get_everything
    raise NewsAPIException(r.json())
newsapi.newsapi_exception.NewsAPIException: {'status': 'error', 'code': 'parameterInvalid', 'message': 'You are trying to request results too far in the past. Your plan permits you to request articles as far back as 2025-09-15, but you have requested 2025-08-29. You may need to upgrade to a paid plan.'}
2025-10-16T10:25:34.593Z - INFO - --- Releasing FinBERT model ---
2025-10-16T10:25:34.617Z - INFO - --- Loading and Running Instances 2b & 3: Llama Synthesis ---
`torch_dtype` is deprecated! Use `dtype` instead!
2025-10-16T10:25:35.049Z - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [01:01<01:01, 61.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:33<00:00, 44.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:33<00:00, 46.72s/it]
2025-10-16T10:27:11.272Z - INFO - --- [STEP 5] Generating 'as-if' analyst report ---
2025-10-16T10:27:35.682Z - INFO - --- [STEP 6] Generating final test performance review ---
2025-10-16T10:28:06.374Z - INFO - --- Releasing all models from memory ---
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   2274 MiB |   3377 MiB |   1126 GiB |   1124 GiB |
|       from large pool |   2231 MiB |   3345 MiB |   1016 GiB |   1014 GiB |
|       from small pool |     42 MiB |    130 MiB |    109 GiB |    109 GiB |
|---------------------------------------------------------------------------|
| Active memory         |   2274 MiB |   3377 MiB |   1126 GiB |   1124 GiB |
|       from large pool |   2231 MiB |   3345 MiB |   1016 GiB |   1014 GiB |
|       from small pool |     42 MiB |    130 MiB |    109 GiB |    109 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |   2272 MiB |   3375 MiB |   1106 GiB |   1104 GiB |
|       from large pool |   2229 MiB |   3343 MiB |    997 GiB |    994 GiB |
|       from small pool |     42 MiB |    130 MiB |    109 GiB |    109 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   2514 MiB |   3546 MiB |   6140 MiB |   3626 MiB |
|       from large pool |   2444 MiB |   3440 MiB |   5884 MiB |   3440 MiB |
|       from small pool |     70 MiB |    134 MiB |    256 MiB |    186 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory | 241575 KiB |   1450 MiB |   1262 GiB |   1262 GiB |
|       from large pool | 217472 KiB |   1418 MiB |   1139 GiB |   1139 GiB |
|       from small pool |  24103 KiB |     49 MiB |    123 GiB |    123 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     651    |    1357    |    3768 K  |    3767 K  |
|       from large pool |     282    |     424    |     351 K  |     351 K  |
|       from small pool |     369    |    1276    |    3416 K  |    3416 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     651    |    1357    |    3768 K  |    3767 K  |
|       from large pool |     282    |     424    |     351 K  |     351 K  |
|       from small pool |     369    |    1276    |    3416 K  |    3416 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      51    |      84    |     152    |     101    |
|       from large pool |      16    |      17    |      24    |       8    |
|       from small pool |      35    |      67    |     128    |      93    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     106    |     113    |    1793 K  |    1793 K  |
|       from large pool |      10    |      19    |     130 K  |     130 K  |
|       from small pool |      96    |     105    |    1662 K  |    1662 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

INFO:     127.0.0.1:49362 - "POST /test HTTP/1.1" 200 OK
